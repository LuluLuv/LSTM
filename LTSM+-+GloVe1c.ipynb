{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\willi10l\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\willi10l\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: enum34>=1.1.6 in c:\\users\\willi10l\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\users\\willi10l\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: protobuf>=3.3.0 in c:\\users\\willi10l\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\willi10l\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in c:\\users\\willi10l\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: setuptools in c:\\users\\willi10l\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\setuptools-27.2.0-py3.6.egg (from protobuf>=3.3.0->tensorflow)\n",
      "Requirement already satisfied: bleach==1.5.0 in c:\\users\\willi10l\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\n",
      "Requirement already satisfied: html5lib==0.9999999 in c:\\users\\willi10l\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\users\\willi10l\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\willi10l\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.contrib import rnn\n",
    "import os\n",
    "import zipfile\n",
    "from scipy import spatial\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'C://Users//willi10l//Downloads//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "\n",
    "vocab_fraction = .00005\n",
    "hidden_layer = 512\n",
    "embedding_dim = 100\n",
    "time_steps = 3\n",
    "learning_rate = .001\n",
    "\n",
    "#training parameters\n",
    "\n",
    "num_iters = 10000\n",
    "display_steps = 500\n",
    "\n",
    "log_dir = 'C://Users//willi10l/Downloads'\n",
    "run_name = 'rundemo -' + str(vocab_fraction) + '-' + str(hidden_layer) + '-' + str(num_iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file loaded\n",
      "'the' found\n",
      "1020\n"
     ]
    }
   ],
   "source": [
    "zip_path = 'C://Users//willi10l//Downloads//glove.6B.zip'\n",
    "pkl_path = 'C://Users//willi10l//Downloads//glove.6B.pkl'\n",
    "\n",
    "def create_corpus(zip_path, pkl_path):\n",
    "    vocab = []\n",
    "    with zipfile.ZipFile(zip_path) as f1:\n",
    "        with f1.open(f1.namelist()[0]) as bfile:\n",
    "            all_data = bfile.read().strip().split()\n",
    "        \n",
    "            for word in all_data:\n",
    "                vocab.append(word.decode('UTF-8'))\n",
    "                \n",
    "#serialize the data and put it into a pickle file which can be retrieved later\n",
    "\n",
    "    with open(pkl_path, 'wb') as f2:\n",
    "        pickle.dump(vocab, f2, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return vocab\n",
    "\n",
    "if Path(pkl_path).is_file():\n",
    "    \n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "        print(\"Pickle file loaded\")\n",
    "        \n",
    "else:\n",
    "    corpus = create_corpus(zip_path, pkl_path)\n",
    "    \n",
    "trunc_corpus = corpus[:int(math.floor(len(corpus) * vocab_fraction))]\n",
    "\n",
    "if 'the' in trunc_corpus:\n",
    "    print(\"'the' found\")\n",
    "    \n",
    "print(len(trunc_corpus))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '0.418', '0.24968', '-0.41242', '0.1217', '0.34527', '-0.044457', '-0.49688', '-0.17862', '-0.00066023', '-0.6566', '0.27843', '-0.14767', '-0.55677', '0.14658', '-0.0095095', '0.011658', '0.10204', '-0.12792', '-0.8443', '-0.12181', '-0.016801', '-0.33279', '-0.1552', '-0.23131', '-0.19181', '-1.8823', '-0.76746', '0.099051', '-0.42125', '-0.19526', '4.0071', '-0.18594', '-0.52287', '-0.31681', '0.00059213', '0.0074449', '0.17778', '-0.15897', '0.012041', '-0.054223', '-0.29871', '-0.15749', '-0.34758', '-0.045637', '-0.44251', '0.18785', '0.0027849', '-0.18411', '-0.11514', '-0.78581', ',', '0.013441', '0.23682', '-0.16899', '0.40951', '0.63812', '0.47709', '-0.42852', '-0.55641', '-0.364', '-0.23938', '0.13001', '-0.063734', '-0.39575', '-0.48162', '0.23291', '0.090201', '-0.13324', '0.078639', '-0.41634', '-0.15428', '0.10068', '0.48891', '0.31226', '-0.1252', '-0.037512', '-1.5179', '0.12612', '-0.02442', '-0.042961', '-0.28351', '3.5416', '-0.11956', '-0.014533', '-0.1499', '0.21864', '-0.33412', '-0.13872', '0.31806', '0.70358', '0.44858', '-0.080262', '0.63003', '0.32111', '-0.46765', '0.22786', '0.36034', '-0.37818', '-0.56657', '0.044691', '0.30392', '.', '0.15164', '0.30177', '-0.16763', '0.17684', '0.31719', '0.33973', '-0.43478', '-0.31086', '-0.44999', '-0.29486', '0.16608', '0.11963', '-0.41328', '-0.42353', '0.59868', '0.28825', '-0.11547', '-0.041848', '-0.67989', '-0.25063', '0.18472', '0.086876', '0.46582', '0.015035', '0.043474', '-1.4671', '-0.30384', '-0.023441', '0.30589', '-0.21785', '3.746', '0.0042284', '-0.18436', '-0.46209', '0.098329', '-0.11907', '0.23919', '0.1161', '0.41705', '0.056763', '-6.3681e-05', '0.068987', '0.087939', '-0.10285', '-0.13931', '0.22314', '-0.080803', '-0.35652', '0.016413', '0.10216', 'of', '0.70853', '0.57088', '-0.4716', '0.18048', '0.54449', '0.72603', '0.18157', '-0.52393', '0.10381', '-0.17566', '0.078852', '-0.36216', '-0.11829', '-0.83336', '0.11917', '-0.16605', '0.061555', '-0.012719', '-0.56623', '0.013616', '0.22851', '-0.14396', '-0.067549', '-0.38157', '-0.23698', '-1.7037', '-0.86692', '-0.26704', '-0.2589', '0.1767', '3.8676', '-0.1613', '-0.13273', '-0.68881', '0.18444', '0.0052464', '-0.33874', '-0.078956', '0.24185', '0.36576', '-0.34727', '0.28483', '0.075693', '-0.062178', '-0.38988', '0.22902', '-0.21617', '-0.22562', '-0.093918', '-0.80375', 'to', '0.68047', '-0.039263', '0.30186', '-0.17792', '0.42962', '0.032246', '-0.41376', '0.13228', '-0.29847', '-0.085253', '0.17118', '0.22419', '-0.10046', '-0.43653', '0.33418', '0.67846', '0.057204', '-0.34448', '-0.42785', '-0.43275', '0.55963', '0.10032', '0.18677', '-0.26854', '0.037334', '-2.0932', '0.22171', '-0.39868', '0.20912', '-0.55725', '3.8826', '0.47466', '-0.95658', '-0.37788', '0.20869', '-0.32752', '0.12751', '0.088359', '0.16351', '-0.21634', '-0.094375', '0.018324', '0.21048', '-0.03088', '-0.19722', '0.082279', '-0.09434', '-0.073297', '-0.064699', '-0.26044', 'and', '0.26818', '0.14346', '-0.27877', '0.016257', '0.11384', '0.69923', '-0.51332', '-0.47368', '-0.33075', '-0.13834', '0.2702', '0.30938', '-0.45012', '-0.4127', '-0.09932', '0.038085', '0.029749', '0.10076', '-0.25058', '-0.51818', '0.34558', '0.44922', '0.48791', '-0.080866', '-0.10121', '-1.3777', '-0.10866', '-0.23201', '0.012839', '-0.46508', '3.8463', '0.31362', '0.13643', '-0.52244', '0.3302', '0.33707', '-0.35601', '0.32431', '0.12041', '0.3512', '-0.069043', '0.36885', '0.25168', '-0.24517', '0.25381', '0.1367', '-0.31178', '-0.6321', '-0.25028', '-0.38097', 'in', '0.33042', '0.24995', '-0.60874', '0.10923', '0.036372', '0.151', '-0.55083', '-0.074239', '-0.092307', '-0.32821', '0.09598', '-0.82269', '-0.36717', '-0.67009', '0.42909', '0.016496', '-0.23573', '0.12864', '-1.0953', '0.43334', '0.57067', '-0.1036', '0.20422', '0.078308', '-0.42795', '-1.7984', '-0.27865', '0.11954', '-0.12689', '0.031744', '3.8631', '-0.17786', '-0.082434', '-0.62698', '0.26497', '-0.057185', '-0.073521', '0.46103', '0.30862', '0.12498', '-0.48609', '-0.0080272', '0.031184', '-0.36576', '-0.42699', '0.42164', '-0.11666', '-0.50703', '-0.027273', '-0.53285', 'a', '0.21705', '0.46515', '-0.46757', '0.10082', '1.0135', '0.74845', '-0.53104', '-0.26256', '0.16812', '0.13182', '-0.24909', '-0.44185', '-0.21739', '0.51004', '0.13448', '-0.43141', '-0.03123', '0.20674', '-0.78138', '-0.20148', '-0.097401', '0.16088', '-0.61836', '-0.18504', '-0.12461', '-2.2526', '-0.22321', '0.5043', '0.32257', '0.15313', '3.9636', '-0.71365', '-0.67012', '0.28388', '0.21738', '0.14433', '0.25926', '0.23434', '0.4274', '-0.44451', '0.13813', '0.36973', '-0.64289', '0.024142', '-0.039315', '-0.26037', '0.12017', '-0.043782', '0.41013', '0.1796', '\"', '0.25769', '0.45629', '-0.76974', '-0.37679', '0.59272', '-0.063527', '0.20545', '-0.57385', '-0.29009', '-0.13662', '0.32728', '1.4719', '-0.73681', '-0.12036', '0.71354', '-0.46098', '0.65248', '0.48887', '-0.51558', '0.039951', '-0.34307', '-0.014087', '0.86488', '0.3546', '0.7999', '-1.4995', '-1.8153', '0.41128', '0.23921', '-0.43139', '3.6623', '-0.79834', '-0.54538', '0.16943', '-0.82017', '-0.3461', '0.69495', '-1.2256', '-0.17992', '-0.057474', '0.030498', '-0.39543', '-0.38515', '-1.0002', '0.087599', '-0.31009', '-0.34677', '-0.31438', '0.75004', '0.97065', \"'s\", '0.23727', '0.40478', '-0.20547', '0.58805', '0.65533', '0.32867', '-0.81964', '-0.23236', '0.27428', '0.24265', '0.054992', '0.16296', '-1.2555', '-0.086437', '0.44536', '0.096561', '-0.16519', '0.058378', '-0.38598', '0.086977', '0.0033869', '0.55095', '-0.77697', '-0.62096', '0.092948', '-2.5685', '-0.67739', '0.10151', '-0.48643', '-0.057805', '3.1859', '-0.017554', '-0.16138', '0.055486', '-0.25885', '-0.33938', '-0.19928', '0.26049', '0.10478', '-0.55934', '-0.12342', '0.65961', '-0.51802', '-0.82995', '-0.082739', '0.28155', '-0.423', '-0.27378', '-0.007901', '-0.030231', 'for', '0.15272', '0.36181', '-0.22168', '0.066051', '0.13029', '0.37075', '-0.75874', '-0.44722', '0.22563', '0.10208', '0.054225', '0.13494', '-0.43052', '-0.2134', '0.56139', '-0.21445', '0.077974', '0.10137', '-0.51306', '-0.40295', '0.40639', '0.23309', '0.20696', '-0.12668', '-0.50634', '-1.7131', '0.077183', '-0.39138', '-0.10594', '-0.23743', '3.9552', '0.66596', '-0.61841', '-0.3268', '0.37021', '0.25764', '0.38977', '0.27121', '0.043024', '-0.34322', '0.020339', '0.2142', '0.044097', '0.14003', '-0.20079', '0.074794', '-0.36076', '0.43382', '-0.084617', '0.1214', '-', '-0.16768', '1.2151', '0.49515', '0.26836', '-0.4585', '-0.23311', '-0.52822', '-1.3557', '0.16098', '0.37691', '-0.92702', '-0.43904', '-1.0634', '1.028', '0.0053943', '0.04153', '-0.018638', '-0.55451', '0.026166', '0.28066', '-0.66245', '0.23435', '0.2451', '0.025668', '-1.0869', '-2.844', '-0.51272', '0.27286', '0.0071502', '0.033984', '3.9084', '0.52766', '-0.66899', '1.8238', '0.43436', '-0.30084', '-0.26996', '0.4394', '0.69956', '0.14885', '0.029453', '1.4888', '0.52361', '0.099354', '1.2515', '0.099381', '-0.079261', '-0.30862', '0.30893', '0.11023', 'that', '0.88387', '-0.14199', '0.13566', '0.098682', '0.51218', '0.49138', '-0.47155', '-0.30742', '0.01963', '0.12686', '0.073524', '0.35836', '-0.60874', '-0.18676', '0.78935', '0.54534', '0.1106', '-0.2923', '0.059041', '-0.69551', '-0.18804', '0.19455', '0.32269', '-0.49981', '0.306', '-2.3902', '-0.60749', '0.37107', '0.078912', '-0.23896', '3.839', '-0.20355', '-0.35613', '-0.69185', '-0.17497', '-0.35323', '0.10598', '-0.039303', '0.015701', '0.038279', '-0.35283', '0.44882', '-0.16534', '0.31579', '0.14963', '-0.071277', '-0.53506', '0.52711', '-0.20148', '0.0095952', 'on', '0.30045', '0.25006', '-0.16692', '0.1923', '0.026921', '-0.079486', '-0.91383', '-0.1974', '-0.053413', '-0.40846', '-0.26844', '-0.28212', '-0.5', '0.1221', '0.3903', '0.17797', '-0.4429', '-0.40478', '-0.9505', '-0.16897', '0.77793', '0.33525', '0.3346', '-0.1754', '-0.12017', '-1.7861', '0.29241', '0.55933', '0.029982', '-0.32417', '3.9297', '0.1088', '-0.57335', '-0.17842', '0.0041748', '-0.16309', '0.45077', '-0.16123', '-0.17311', '-0.087889', '-0.089032', '0.062001', '-0.19946', '-0.38863', '-0.18232', '0.060751', '0.098603', '-0.07131', '0.23052', '-0.51939', 'is', '0.6185', '0.64254', '-0.46552', '0.3757', '0.74838', '0.53739', '0.0022239', '-0.60577', '0.26408', '0.11703', '0.43722', '0.20092', '-0.057859', '-0.34589', '0.21664', '0.58573', '0.53919', '0.6949', '-0.15618', '0.05583', '-0.60515', '-0.28997', '-0.025594', '0.55593', '0.25356', '-1.9612', '-0.51381', '0.69096', '0.066246', '-0.054224', '3.7871', '-0.77403', '-0.12689', '-0.51465', '0.066705', '-0.32933', '0.13483', '0.19049', '0.13812', '-0.21503', '-0.016573', '0.312', '-0.33189', '-0.026001', '-0.38203', '0.19403', '-0.12466', '-0.27557', '0.30899', '0.48497', 'was', '0.086888', '-0.19416', '-0.24267', '-0.33391', '0.56731', '0.39783', '-0.97809', '0.03159', '-0.61469', '-0.31406', '0.56145', '0.12886', '-0.84193', '-0.46992', '0.47097', '0.023012', '-0.59609', '0.22291', '-1.1614', '0.3865', '0.067412', '0.44883', '0.17394', '-0.53574', '0.17909', '-2.1647', '-0.12827', '0.29036', '-0.15061', '0.35242', '3.124', '-0.90085', '-0.02567', '-0.41709', '0.40565', '-0.22703', '0.76829', '0.60982', '0.070068', '-0.13271', '-0.1201', '0.096132', '-0.43998', '-0.48531', '-0.5188', '-0.3077', '-0.75028', '-0.77', '0.3945', '-0.16937', 'said', '0.38973', '-0.2121', '0.51837', '0.80136', '1.0336', '-0.27784', '-0.84525', '-0.25333', '0.12586', '-0.90342', '0.24975', '0.22022', '-1.2053', '-0.53771', '1.0446', '0.62778', '0.39704', '-0.15812', '0.38102', '-0.54674', '-0.44009', '1.0976', '0.013069', '-0.89971', '0.41226', '-2.2309', '0.28997', '0.32175', '-0.72738', '-0.092244', '3.028', '-0.062599', '0.038329', '0.0072918', '-0.35388', '-0.92256', '0.097932', '0.10068', '1.2116', '0.88233', '-0.46297', '1.3186', '0.32705', '-0.73446', '0.89301', '-0.45324', '-1.2698', '0.86119', '0.1415', '1.2018', 'with', '0.25616', '0.43694', '-0.11889', '0.20345', '0.41959', '0.85863', '-0.60344', '-0.31835', '-0.6718', '0.003984', '-0.075159', '0.11043', '-0.73534', '0.27436', '0.054015', '-0.23828', '-0.13767', '0.011573', '-0.46623', '-0.55233', '0.083317', '0.55938', '0.51903', '-0.27065', '-0.28211', '-1.3918', '0.17498', '0.26586', '0.061449', '-0.273', '3.9032', '0.38169', '-0.056009', '-0.004425', '0.24033', '0.30675', '-0.12638', '0.33436', '0.075485', '-0.036218', '0.13691', '0.37762', '-0.12159', '-0.13808', '0.19505', '0.22793', '-0.17304', '-0.07573', '-0.25868', '-0.39339', 'he', '-0.20092', '-0.060271', '-0.61766', '-0.8444', '0.5781', '0.14671', '-0.86098', '0.6705', '-0.86556', '-0.18234', '0.15856', '0.45814', '-1.0163', '-0.35874', '0.73869', '-0.24048', '-0.33893', '0.25742', '-0.78192', '0.083528', '0.1775', '0.91773', '0.64531', '-0.19896', '0.37416', '-2.7525', '-0.091586', '0.040349', '-0.064792', '-0.31466', '3.3944', '0.044941', '-0.55038', '-0.65334', '0.10436', '0.016394', '0.24388', '1.0085', '0.31412', '-0.33806', '-0.16925', '0.10228', '-0.62143', '0.19829', '-0.36147', '-0.24769', '-0.38989', '-0.33317', '-0.041659', '-0.013171', 'as', '0.20782', '0.12713', '-0.30188', '-0.23125', '0.30175', '0.33194', '-0.52776', '-0.44042', '-0.48348', '0.03502', '0.34782', '0.54574', '-0.2066', '-0.083713', '0.2462', '0.15931', '-0.0031349', '0.32443', '-0.4527', '-0.22178', '0.022652', '-0.041714', '0.31815', '0.088633', '-0.03801', '-1.8212', '-0.50917', '-0.097544', '-0.08953', '0.050476', '3.718', '-0.16503', '-0.078733', '-0.57101', '0.20418', '0.13411', '0.074281', '0.087502', '-0.25443', '-0.15011', '-0.15768', '0.39606', '-0.23646', '-0.095054', '0.07859', '-0.012305', '-0.49879', '-0.35301', '0.05058', '0.019495']\n"
     ]
    }
   ],
   "source": [
    "print(trunc_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "glove_dir = 'C://Users//willi10l//Downloads//'\n",
    "glove_file = 'glove.6B.100d'\n",
    "glove_path = glove_dir + glove_file + '.txt'\n",
    "\n",
    "with open (glove_path) as myfile:\n",
    "    head = [next(myfile) for x in range(3)]\n",
    "print(head[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found files\n",
      "loaded vocab file\n",
      "loaded embed file\n",
      "['the']\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "#create vocab from teh GloVe embedding file\n",
    "\n",
    "vocab_pkl = glove_dir + glove_file + '.vpkl'\n",
    "embed_pkl = glove_dir + glove_file + '.mpkl'\n",
    "\n",
    "def create_glove_vocab(path, v_pkl, e_pkl):\n",
    "    vocab = []\n",
    "    embedding_dict = {}\n",
    "    \n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            data = line.strip().split(' ')\n",
    "            vocab.append(data[0])\n",
    "            embedding_dict[data[0]] = [float(val) for val in data [1:]]\n",
    "            \n",
    "    with open(v_pkl, 'wb') as f1:\n",
    "        pickle.dump(vocab, f1, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"created vocab file\")\n",
    "        \n",
    "    with open(e_pkl, 'wb') as f2:\n",
    "        pickle.dump(embedding_dict, f2, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"created embed file\")\n",
    "        \n",
    "    return vocab, embedding_dict\n",
    "\n",
    "if Path(vocab_pkl).is_file() and Path(embed_pkl).is_file():\n",
    "    print(\"found files\")\n",
    "    with open(vocab_pkl, 'rb') as f1:\n",
    "        glove_vocab = pickle.load(f1)\n",
    "        print(\"loaded vocab file\")\n",
    "        \n",
    "    with open(embed_pkl, 'rb') as f2:\n",
    "        embedding_dict = pickle.load(f2)\n",
    "        print(\"loaded embed file\")\n",
    "        \n",
    "else:\n",
    "    glove_vocab, embedding_dict = create_glove_vocab(glove_path, vocab_pkl, embed_pkl)\n",
    "    \n",
    "    \n",
    "print(glove_vocab[:1])\n",
    "print(len(glove_vocab))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "#create dictionary of training corpus\n",
    "\n",
    "def create_dictionary(training_data):\n",
    "    dictionary = {}\n",
    "    count = collections.Counter(training_data). most_common()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary.items())\n",
    "    reverse_dict = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return dictionary, reverse_dict\n",
    "\n",
    "dictionary, reverse_dict = create_dictionary(data_path)\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#some words in corpus may not have embeddings in GloVe. So we create random initial embeddings for those words\n",
    "#create embeddings specific to training vocab\n",
    "\n",
    "def create_embeds(training_dict, glove_vocab, embedding_dict):\n",
    "    input_embeds = []\n",
    "    exceptions = {}\n",
    "    for word, i in training_dict.items():\n",
    "        if word in glove_vocab:\n",
    "            input_embeds.append(embedding_dict[word])\n",
    "        else:\n",
    "            new_embed = np.random.uniform(low=-0.2, high=0.2, size=embedding_dim)\n",
    "            input_embeds.append(new_embed)\n",
    "            exceptions.append(word)\n",
    "        return input_embeds, exceptions\n",
    "    \n",
    "input_embeds, exceptions = create_embeds(dictionary, glove_vocab, embedding_dict)\n",
    "input_embeds = np.asarray(input_embeds)\n",
    "\n",
    "#creating a spatial representation of the embeddings. This can be queried\n",
    "tree = spatial.KDTree(input_embeds)\n",
    "\n",
    "print(len(exceptions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Failed to convert object of type <class 'set'> to Tensor. Contents: {100}. Consider casting elements to a supported type.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\willi10l\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m       \u001b[0mstr_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\willi10l\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m       \u001b[0mstr_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\willi10l\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[1;32m---> 65\u001b[1;33m                     (bytes_or_text,))\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected binary or unicode string, got {100}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-b4cea2680c5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'outputs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mout_wts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'out_wts'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mout_biases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'out_biases'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'embedding'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\willi10l\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\u001b[0m in \u001b[0;36mrandom_normal\u001b[1;34m(shape, mean, stddev, dtype, seed, name)\u001b[0m\n\u001b[0;32m     69\u001b[0m   \"\"\"\n\u001b[0;32m     70\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"random_normal\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mshape_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ShapeTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mmean_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mstddev_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstddev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"stddev\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\willi10l\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\u001b[0m in \u001b[0;36m_ShapeTensor\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m     40\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\willi10l\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[0;32m    834\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 836\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\willi10l\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\willi10l\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    227\u001b[0m                                          as_ref=False):\n\u001b[0;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\willi10l\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m    206\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m--> 208\u001b[1;33m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[1;32mC:\\Users\\willi10l\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    470\u001b[0m       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\u001b[0;32m    471\u001b[0m                       \u001b[1;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[0;32m    473\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Failed to convert object of type <class 'set'> to Tensor. Contents: {100}. Consider casting elements to a supported type."
     ]
    }
   ],
   "source": [
    "#Create LSTM model in tensorflow\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    \n",
    "    x_id = tf.placeholder(dtype=tf.int32, shape=(None, time_steps), name='input_words')\n",
    "    y_label = tf.placeholder(dtype=tf.float32, shape=(None, embedding_dim), name='target_embed')\n",
    "    \n",
    "with tf.name_scope('outputs'):\n",
    "    out_wts = tf.Variable(tf.random_normal(shape=([hidden_layer, embedding_dim])), name='out_wts')\n",
    "    out_biases = tf.Variable(tf.random_normal(shape=({embedding_dim})), name='out_biases')\n",
    "\n",
    "with tf.name_scope('embedding'):\n",
    "    \n",
    "    #make the embedding trainable\n",
    "    \n",
    "    embed_wts = tf.Variable(tf.constant(0.0, shape=[len(dictionary), embedding_dim]), trainable=True, name='embed_wts')\n",
    "    embeddings = tf.placeholder(dtype=tf.float32, shape=(len(dictionary), embedding_dim), name='input_embeds')\n",
    "    \n",
    "    #initialize with glove + home made embeddings\n",
    "    \n",
    "    embed_init = embed_wts.assign(embeddings)\n",
    "    \n",
    "    input_embed = tf.nn.embedding_lookup(embed_wts, x_id)\n",
    "    \n",
    "with tf.name_scope('LSTM_model'):\n",
    "    \n",
    "    #this is a 2 layer LSTM\n",
    "    \n",
    "    lstm_cell = rnn.BasicLSTMCell(hidden_layer)\n",
    "    lstm_cell = rnnBasicLSTMCell(hidden_layer)\n",
    "    \n",
    "    rnn_cell = rnn.MultiRNNCell({lstm_cell1, lstm_cell2})\n",
    "    \n",
    "    #the inputs are basically time step number of vector (in this case 3\n",
    "    #we untack them in to a matrix of time-step, embed_dim\n",
    "    \n",
    "    x_id_unstack = tf.unstack(input_embed, time_steps, 1)\n",
    "    \n",
    "    outputs, state = tf.nn.static_rnn(rnn_cell, x_id_unstack, dtype=tf.float32)\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    \n",
    "    #define the loss\n",
    "    \n",
    "    out_pred = tf.matmul(outputs[-1], out_wts) + out_biases\n",
    "    loss = tf.reduce_mean(tf.nn.l2_loss(out_pred - y_label))\n",
    "    \n",
    "tf.summary.scalar(\"12_loss\", loss)\n",
    "with tf.name_scope('train_fn'):\n",
    "    \n",
    "    train_step = tf.train.AdamsOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "\n",
    "summ = tf.summary.merge_all()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "training_data = trunc_corpus\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "train_writer = tf.summery.FileWriter(log_dir + run_name, sess.graph)\n",
    "sess.run(init)\n",
    "sess.run(embed_init, fedd_dict={embeddings: input_embeds})\n",
    "\n",
    "steps = 0\n",
    "step_loss = 0\n",
    "acc_loss = 0\n",
    "offset = random.randint(0, time_steps + 1)\n",
    "end_offset = offset + time_steps\n",
    "\n",
    "while steps < num_iter:\n",
    "    if offset > len(training_data) - end_offset:\n",
    "        offset = random.randit(0, time_steps +1)\n",
    "        \n",
    "    x_int = [dictionary[training_data[i]] for i in range(offset, offset + timee_steps)] \n",
    "    \n",
    "    #index of the input word\n",
    "    x_int = np.reshape(x_int, [-1, time_steps])\n",
    "    \n",
    "    y_pos = offset + time_steps #index the prediction word\n",
    "    y_int = dictionary[training_data[y_post]] #index of the word in the dictionary\n",
    "    y_embed = input_embeds[y_int, :] #getting the embedding vector of y which is a list\n",
    "    y_embed = np.reshape(y_embed, [1, -1]) #the list has to be converted into a numpy array for tf to work \n",
    "    _, step_loss, pred, summary = sess.run([train_step, loss, out_pred, summ] feed_dict={x_id: x_int, y_label: y_embed})\n",
    "    train_writer.add_summary(summary, steps)\n",
    "    \n",
    "    acc_loss += step_loss\n",
    "    \n",
    "    if (steps + 1) % display_steps ==0:\n",
    "        input_words = [training_data[i] for i in range(offset, offset + time_steps)]\n",
    "        target_word = {training_data{y_pos}}\n",
    "        \n",
    "    #using KDTree to query for the ids of words which are closest in vector space of the predicted embeddings\n",
    "    \n",
    "        nearest_dist, nearest_idx = tree.query(pred{0}, 3)\n",
    "        nearest_words = {reverse_dict{i} fori in nearest_idx}\n",
    "        \n",
    "        if (steps + 1) == display_steps:\n",
    "            print(pred)\n",
    "            \n",
    "        print(\"Input words [%s} vs target %s vs nearest words [%s)\" % (input_words, target_words, nearest_words))\n",
    "        \n",
    "        acc_loss = 0\n",
    "        \n",
    "    steps += 1\n",
    "    offset += time_steps + 1\n",
    "    \n",
    "#setup the embeddings projector on tensorboard\n",
    "config = projector.ProjectorConfig()\n",
    "embedding_config = config.embeddings.add()\n",
    "embedding_config.tensor_name = 'embeddings/embed_wts' #the name space is very important\n",
    "embedding_conf.metadata_path = os.path.join('/home/ubuntu/nbs/data/demo/', 'metadata.tsv')\n",
    "projector.visualize_embeddings(train_writer, config)\n",
    "\n",
    "#save the model\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join('/home/ubuntu/nbs/data/demo', \"model.ckpt\"))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
